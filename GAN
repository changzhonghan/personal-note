pix2pix
dualGAN
cycleGAN
DIscoGAn

text 2 image
Generative Adversarial Text to Image Synthesis
StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Network

pix2pix 
https://arxiv.org/pdf/1611.07004v1.pdf
http://m.blog.csdn.net/article/details?id=53284559
https://zhuanlan.zhihu.com/p/25000523?refer=dlclass

[0]

    生成对抗网络（GAN：Generative Adversarial Networks）
        GAN启发自博弈论中的二人零和博弈，由[Goodfellow et al, NIPS 2014]开创性地提出，包含一个生成模型（generative model G）和一个判

别模型（discriminative model D）。生成模型捕捉样本数据的分布，判别模型是一个二分类器，判别输入是真实数据还是生成的样本。这个模型的优

化过程是一个“二元极小极大博弈（minimax two-player game）”问题，训练时固定一方，更新另一个模型的参数，交替迭代，使得对方的错误最大化

，最终，G 能估测出样本数据的分布。
    变分自编码器（VAE: Variational Autoencoders）
        在概率图形模型（probabilistic graphical models ）的框架中对这一问题进行形式化——在概率图形模型中，我们在数据的对数似然上最大

化下限（lower bound）。
    自回归模型（Autoregressive models）
        PixelRNN 这样的自回归模型则通过给定的之前的像素（左侧或上部）对每个单个像素的条件分布建模来训练网络。这类似于将图像的像素插入 

char-rnn 中，但该 RNN 在图像的水平和垂直方向上同时运行，而不只是字符的 1D 序列。




[0]
GAN是如何工作的？
原始图
[0]
GAN是如何工作的？
黄色的图
http://www.w2bc.com/article/197155


[1]
生成器  z>>x
使用pz表示输入噪声的后验概率
使用G表示生成器的映射方程
生成器的分布pg
生成器使用神经网络实现

判别器  D(x)>>0-1
使用神经网络构建D
D(x)表示x来自真实数据的可能性
判别器的输出为标量

[2]
方程(1)
生成器和判决器进行minimax博弈
生成器用于生成伪数据,欺骗判别器
判别器用于判别输入是否来自真实数据
生成器和判别器同步训练

[3]
figure 1 a

生成对抗网络训练过程中,同步训练生成器和判别器
生成器不断更新pg拟合pdata
判决器同步更新,判断数据是否来自pg

[4]
figure bcd
迭代中，判别器被训练用于识别数据真伪，并最终收敛
更新生成器的过程中，来自判别器的梯度指导映射G逼近真实数据
经过多次迭代训练，G和D用足够的学习能力，系统最终收敛：pg=pdata d = 1/2


[5]
训练过程
进入循环体，迭代N次
  进入循环体，迭代k次
     根据噪声分布pz，采样噪声数据
     根据样本分布pdatax，采样真实数据
     使用随机梯度下降更新判别器
  根据噪声分布pz，采样噪声数据
  使用随机梯度下降更新生成器


[6]理论验证
命题1 当生成器G固定时，判别器的理想解为：

证明：对于任意更定的生成器G，判别器的作用为最大化V
v= 
[7]
判别器D的优化目标是使田间概率p(y=y|x)最大
x：表示输入判别器的数据，可能来自真实样本或生成器
y：x是否来自真实数据
minimax游戏可以表示为：方程4

[8]
 定理 1
当cg成立时，当且仅当存在pg=pdata
证明：通过化简CG得到：
CD=。。

其中，KL散度( Kullback–Leibler divergence)，又称相对熵（relative entropy)，
当且仅当 pg=pdata时，cg取得最小值

[9]算法收敛
证明. 如上述标准考虑V(G,D)=U(pg,D)为关于pg的函数。注意到U(pg,D)为pg的凸函数。该凸函数上确界的一次导数包括达到最大值处的该函数的导数。

换句话说，如果f(x)=supα∈Afα(x)，且对每个α，fα(x)关于x是凸的，那么如果β=arg1supα∈Afα(x)，则?fβ(x)∈?f。等价于给定对应的G和最

优判别D，梯度下降更新pg。在定理 1 中证明supDU(pg,D)关于pg是凸的且有唯一的全局最优解，因此，pg更新足够小时，pg收敛到px，证毕。

[10]试验
使用了MNIST手写字符数据集
Toronto Face Database数据集
CIFAR-10数据集


[11]使用了Gaussian Parzen window计算生成样本的似然度
使用测试集进行交叉验证，计算6


[12]
7. 结论与未来工作

该框架允许许多直接的扩展：
1) 添加c至G和D的输入，可获得条件的生成模型p(x|c)。
2) 给定x，为预测z，训练任意的网络可学习近似推理。类似于 wake-sleep 算法训练出的推理网络，但训练推理网络时可能要用到训练完成后的固定的

生成网络。
3) 来近似建模所有的条件概率P(xS|x?S)，其中，S为通过训练共享参数的条件模型簇的x的索引。本质上，对抗的网络可用于随机扩展 MP-DBM。
4) 半监督学习：当标签数据有限时，判别网络或推理网络的特征不会提高分类器效果。
5) 效率改善：为协调G和D设计更好的方法，或训练期间确定更好的分布来采样z，从而加速训练。
(1) 将GAN改进为条件产生式模型：这一点最早在GAN公开后的半年就得到了部分解决，即conditional GAN（ARXIV-2014）的工作，该模型实现了给定条

件的数据生成，但现在在各个领域特别是图像和视频相关的生成工作中，也依然有许多对于给定条件生成数据的任务的相关改进与研究；

(2) 改进输入z：不直接用随机噪声信号，而是可以用其它网络根据真实数据x学习一个z，然后再输入G，相当于是对数据x做了一个编码；这一点目前基

本上在多数基于GAN的应用中都被采纳；

(3) 对条件分布建模，由已有数据预测未出现的数据：往这个方向改进的相关工作相对出现较晚，直到2016年才逐步开始有相关工作出现；

(4) 半监督学习：在2015年年底出现了将GAN用于半监督问题的工作；另外，现有的许多GAN工作也都表明通过加入少量类别标签，引入有标签数据的类别

损失度量，不仅功能上实现了半监督学习，同时也有助于GAN的稳定训练；

(5) 提升GAN的训练效率：目前比GAN的训练效率更加要紧的训练稳定性问题还没有得到很好的解决，因此相对来说，目前这一点的研究并不广泛，而且相

比较其它的产生式模型而言，GAN的速度也不算是一个非常“拖后腿”的点。



